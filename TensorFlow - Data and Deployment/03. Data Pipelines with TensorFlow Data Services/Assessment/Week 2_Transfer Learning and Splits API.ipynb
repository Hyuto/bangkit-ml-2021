{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLSSb7Qly6xf"
   },
   "source": [
    "# TRANSFER LEARNING\n",
    "\n",
    "In this exercise, you will use the Tensorflow Dataset's [Splits API](https://www.tensorflow.org/datasets/splits) and its concepts which you looked at in the week 2 lecture videos. Also\n",
    "\n",
    "Also, you will look at some additional ways of loading things using Tensorflow Hub using the [cats_vs_dogs v4 dataset](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs). \n",
    "\n",
    "Finally, you will use transfer learning using a pretrained feature vector from mobilenet to define a new classification model in the end.\n",
    "\n",
    "Upon completion of the exercise, you will have\n",
    "\n",
    "- Loaded a learnt feature set from mobilenet model\n",
    "- Split the cats_vs_dogs dataset in custom train,validation and test sets.\n",
    "- Shuffled and batched the custom sets.\n",
    "- Defined the model which is ready for tranfer learning using the mobilenet feature vector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Import libraries and set up the splits\n",
    "\n",
    "**Note** : The assignment uses TF version `2.1.0` and TFDS version `3.2.1` so if you run this notebook on TF 1.x or some other version of TFDS, some things might not work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n4.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "def check_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "TEMP_DIR = 'tmp2'\n",
    "DATA_DIR = '../../dataset'\n",
    "\n",
    "check_dir(TEMP_DIR)\n",
    "check_dir(DATA_DIR)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load the Mobilenet model and its features\n",
    "\n",
    "The next code block will download the [`mobilenet model`](https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4) from TensorFlow Hub, and use its learned features, extracted as feature_extractor and set to be fine tuned by making them trainable. \n",
    "\n",
    "It is already downloaded for you but feel free to use the commented part to download the latest version from the tfhub.dev website when experimenting on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSW2AcBLuiHv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub \n",
    "\n",
    "model_selection = (\"mobilenet_v2\", 224, 1280) \n",
    "handle_base, pixels, FV_SIZE = model_selection\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "\n",
    "try:\n",
    "    feature_extractor = hub.KerasLayer(os.path.join(TEMP_DIR, 'mobilenet_v2_feature_vector'),\n",
    "                                       input_shape=IMAGE_SIZE + (3,))\n",
    "except OSError:\n",
    "    MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
    "    feature_extractor = hub.KerasLayer(MODULE_HANDLE, input_shape = IMAGE_SIZE + (3,))\n",
    "\n",
    "feature_extractor.trainable = True  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Split the dataset\n",
    "\n",
    "You need to use subsets of the original [cats_vs_dogs](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) data, which is entirely in the 'train' split. I.E. 'train' contains `25000` records with `1738` corrupted images to in total you have `23,262` images.\n",
    "\n",
    "You will split it up to get\n",
    "\n",
    "- The first 10% is as the 'new' training set\n",
    "- The last 10% is as the new validation and test sets, split down the middle \n",
    "    - i.e. the first half of the last 10% is validation (first 5%)\n",
    "    - the second half is test (last 5%)\n",
    "    \n",
    "These 3 recordsets should be called `train_examples`, `validation_examples` and `test_examples` respectively.\n",
    "\n",
    "**Note**: Remember to use `cats_vs_dogs:4.*.*` as dataset because 4.0 support the new Splits API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QUSLZO8IuEtt"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2326\n1163\n1163\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Split the dataset\n",
    "\n",
    "splits = ['train[:10%]', 'train[90%:95%]', 'train[95%:]']\n",
    "    \n",
    "# Remember to use `cats_vs_dogs:4.*.*` \n",
    "# https://www.tensorflow.org/datasets/catalog/cats_vs_dogs\n",
    "    \n",
    "# It has been downloaded for you so use the data_dir parameter \n",
    "# else it will try to download the dataset and give you an error here\n",
    "\n",
    "splits, info = tfds.load('cats_vs_dogs:4.*.*', split = splits, \n",
    "                         data_dir = DATA_DIR, with_info = True)\n",
    "\n",
    "(train_examples, validation_examples, test_examples) = splits\n",
    "\n",
    "# Testing lengths of the data if they are loaded correctly. Do not edit the code below\n",
    "    \n",
    "train_len = len(list(train_examples))\n",
    "validation_len = len(list(validation_examples))\n",
    "test_len = len(list(test_examples))\n",
    "print(train_len)\n",
    "print(validation_len)\n",
    "print(test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Output\n",
    "```\n",
    "2326\n",
    "1163\n",
    "1163\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Shuffle and map the new batches\n",
    "\n",
    "Now, you will take few of the examples from train set and shuffle them initially.\n",
    "\n",
    "Then, you will map a custom function `format_image` formats the image by resizing them first to `(224, 224)` as that is the input image size for mobilenet and post resizing, it normalizes the image by dividing each pixel by 255. \n",
    "\n",
    "Finally, you will create train, test and validation batches with size `16` here because of memory constraints. Do not edit the `BATCH_SIZE` in the code cell and while submitting the assignment.\n",
    "\n",
    "Feel free to play around the `BATCH_SIZE` and other parameters if you are running this locally just for experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtSJorjivpS9"
   },
   "outputs": [],
   "source": [
    "num_examples = 1000 # UPDATE AND USE IT WHEN PLAYING AROUND AND TRAINING IT LOCALLY.\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nkh5t21-uZFs"
   },
   "outputs": [],
   "source": [
    "# EXERCISE: shuffle and map the batches\n",
    "\n",
    "# This will turn the 3 sets into batches\n",
    "# so you can train and load batches\n",
    "\n",
    "def format_image(features):\n",
    "    image = features['image']\n",
    "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
    "    return  image, features['label']\n",
    "    \n",
    "BATCH_SIZE =  16 # DO NOT EDIT\n",
    "\n",
    "# For training batches, shuffle the examples by num_examples, map using the function defined above\n",
    "# and batching using the batch_size.\n",
    "\n",
    "# For validation and test batches, just avoid shuffling and follow the rest as training batch example\n",
    "\n",
    "train_batches = train_examples.map(format_image).shuffle(num_examples).batch(BATCH_SIZE)\n",
    "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE)\n",
    "test_batches = test_examples.map(format_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Define your transfer learning model\n",
    "\n",
    "Here, you will use the mobilenet feature vector which you loaded before from Tensorflow Hub to create a new model for training. \n",
    "\n",
    "This is a simple model where you are just using the feature vectors and adding the final dense layer to get the cat/dog classification.\n",
    "\n",
    "**Note**: Always be careful of the input and output dimensions for a model loaded for transfer learning and use summary to check the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rmyQ207suyGY"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nkeras_layer (KerasLayer)     (None, 1280)              2257984   \n_________________________________________________________________\ndense (Dense)                (None, 2)                 2562      \n=================================================================\nTotal params: 2,260,546\nTrainable params: 2,226,434\nNon-trainable params: 34,112\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# EXERCISE: Define the model\n",
    "\n",
    "# The new model will take the features from the mobilenet via transfer learning\n",
    "# And add a new dense layer at the bottom, with 2 classes -- for cats and dogs\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor, # USE the feature extractor which you loaded before\n",
    "    tf.keras.layers.Dense(2, activation = 'softmax') # Define a keras dense layer with 2 classes and softmax activation\n",
    "])\n",
    "\n",
    "# Model summary to test your model layers, output shape and number of parameters.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output \n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 2)                 2562      \n",
    "=================================================================\n",
    "Total params: 2,260,546\n",
    "Trainable params: 2,226,434\n",
    "Non-trainable params: 34,112\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Step 5 - Training your model\n",
    "\n",
    "Training is not in the scope of this assignment but you can go ahead and train the network to achieve decent accuracy of 90% and above by training for epochs less than 5. \n",
    "\n",
    "**Note**:This would take quite a lot of time to train on CPU (30-40 minutes per epoch) while here it can take 2-3 minutes per epoch. \n",
    "\n",
    "***Remember to submit your assignment before you uncomment and run the next cells.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLIwqtilvBcN"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0820s vs `on_train_batch_end` time: 0.1550s). Check your callbacks.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0820s vs `on_train_batch_end` time: 0.1550s). Check your callbacks.\n",
      "146/146 - 39s - loss: 0.3835 - acc: 0.8856 - val_loss: 0.3934 - val_acc: 0.8994\n",
      "Epoch 2/5\n",
      "146/146 - 37s - loss: 0.2705 - acc: 0.9261 - val_loss: 0.5357 - val_acc: 0.9011\n",
      "Epoch 3/5\n",
      "146/146 - 37s - loss: 0.2313 - acc: 0.9471 - val_loss: 0.2717 - val_acc: 0.9450\n",
      "Epoch 4/5\n",
      "146/146 - 37s - loss: 0.1904 - acc: 0.9699 - val_loss: 0.2479 - val_acc: 0.9398\n",
      "Epoch 5/5\n",
      "146/146 - 37s - loss: 0.1559 - acc: 0.9755 - val_loss: 0.8027 - val_acc: 0.8323\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with adam optimizer and sparse categorical crossentropy, \n",
    "# and track the accuracy metric\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train it for a number of epochs. You should not need many (max 5)\n",
    "# Train on the train_batches, and validation on the validation_batches \n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "history = model.fit(train_batches, epochs=EPOCHS, validation_data=validation_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jkG0zBHvEnP"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "73/73 [==============================] - 3s 44ms/step - loss: 0.7143 - acc: 0.8392\n",
      "loss: 0.7143\n",
      "acc: 0.8392\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate the model on the test batches\n",
    "eval_results = model.evaluate(test_batches)\n",
    "\n",
    "# And print the results. \n",
    "for metric, value in zip(model.metrics_names, eval_results):\n",
    "    print(metric + ': {:.4}'.format(value))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "tensorflow-datasets-w2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}